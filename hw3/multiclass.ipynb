{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "862658de",
   "metadata": {},
   "source": [
    "## Import statements and loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2defa97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, GlobalMaxPooling2D, BatchNormalization, Activation\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.utils import class_weight\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c5f371",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('bird_spectrograms.hdf5', 'r')\n",
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871136d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(f.keys()):\n",
    "    print(f[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abaab8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds to ensure reproducibility \n",
    "np.random.seed(5322)\n",
    "tf.random.set_seed(5322)\n",
    "random.seed(5322)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdaa056",
   "metadata": {},
   "source": [
    "## Multiclass Classification (All species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96aaac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "# loop through each species' data\n",
    "for species in list(f.keys()):\n",
    "    spectrograms = np.array(f[species])  # (128, 517, N)\n",
    "    n_samples = spectrograms.shape[2]\n",
    "    labels = [species] * n_samples\n",
    "\n",
    "    X_list.append(spectrograms)\n",
    "    y_list.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d7a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack and reshape\n",
    "X = np.concatenate(X_list, axis = 2)      # (128, 517, total_samples)\n",
    "X = np.transpose(X, (2, 0, 1))           # (samples, 128, 517)\n",
    "X = X[..., np.newaxis]                   # (samples, 128, 517, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f768e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels to arrays\n",
    "y = np.array(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945487ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode species to ints\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)          # e.g., 'amecro' → 0, 'amerob' → 1, etc.\n",
    "label_map = dict(zip(le.transform(le.classes_), le.classes_))\n",
    "num_classes = len(le.classes_)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7869118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode\n",
    "y_onehot = to_categorical(y_encoded)     # shape: (samples, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e4ae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize per spectrogram to [0, 1]\n",
    "X_min = np.min(X, axis = (1, 2, 3), keepdims = True)\n",
    "X_max = np.max(X, axis = (1, 2, 3), keepdims = True)\n",
    "X = (X - X_min) / (X_max - X_min + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ccaee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified split to get aligned train/test splits for both X, onehot labels, and raw integer labels\n",
    "X_train, X_test, y_train, y_test, y_train_raw, y_test_raw = train_test_split(\n",
    "    X, y_onehot, y_encoded, test_size = 0.2, random_state = 5322, stratify = y_encoded\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3452ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tried data augmentation by shifting and filling. rotating did not make sense so did not do that\n",
    "datagen = ImageDataGenerator(\n",
    "    width_shift_range = 0.1,\n",
    "    height_shift_range = 0.1,\n",
    "    horizontal_flip = False,\n",
    "    vertical_flip = False,\n",
    "    fill_mode = 'nearest'\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635cd44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_labels = np.argmax(y_train, axis = 1)\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "    class_weight = 'balanced',\n",
    "    classes = np.unique(y_train_labels),\n",
    "    y = y_train_labels\n",
    ")\n",
    "\n",
    "class_weights_dict = dict(enumerate(class_weights))\n",
    "class_weights_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af539dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check class distribution and weights\n",
    "print(\"Training set class distribution:\")\n",
    "print(collections.Counter(y_train_labels))\n",
    "print(\"\\nComputed class weights:\")\n",
    "print(class_weights_dict)\n",
    "\n",
    "# number of classes\n",
    "print(f\"\\nNumber of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082b336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first multiclass model\n",
    "model = Sequential([\n",
    "    # layer group 1\n",
    "    Conv2D(32, (3, 3), input_shape = (128, 517, 1), kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.2),\n",
    "    # group 2\n",
    "    Conv2D(64, (3, 3), kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.3),\n",
    "    # group 3\n",
    "    Conv2D(64, (3, 3), kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.3),\n",
    "    # group 4\n",
    "    Conv2D(128, (3, 3), kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.4),\n",
    "    # group 5\n",
    "    Conv2D(128, (3, 3), kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.4),\n",
    "    # global pooling\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(units = 128, kernel_regularizer = l2(0.0001)),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(units = num_classes, activation = 'softmax')\n",
    "], name = \"Multiclass_Model_1\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer = Adam(learning_rate = 0.0005),\n",
    "    loss = CategoricalCrossentropy(label_smoothing = 0.05),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)\n",
    "\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size = 64),\n",
    "    validation_data = (X_test, y_test),\n",
    "    epochs = 100,\n",
    "    callbacks = [early_stopping],\n",
    "    class_weight = class_weights_dict\n",
    ")\n",
    "\n",
    "# result\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'\\nTest accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803dd151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# get class labels in correct order (by integer encoding)\n",
    "target_names = [label_map[i] for i in sorted(label_map.keys())]\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names = target_names, digits = 4, zero_division = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123f1934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "species_names = list(f.keys())\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize = (12, 10))\n",
    "sns.heatmap(cm, annot = True, fmt = 'd', cmap = 'Blues', xticklabels = species_names, yticklabels = species_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xticks(rotation = 45, ha = 'right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75caf9d",
   "metadata": {},
   "source": [
    "This model performs terribly\\\n",
    "Accuracy is low, AUC is low, the classification report shows only two classes (which arent even the majority class) being predicted\\\n",
    "Need to make drastic changes maybe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4615b15c",
   "metadata": {},
   "source": [
    "## Multiclass Classification with adjustments\n",
    "- Made the model a bit simpler with less layers\n",
    "- Started at 64 instead of 32 filters\n",
    "- Removed dropout in first 2 layers to maybe allow more learning since the previous model did not learn much\n",
    "- Use global max pooling 2d instead of global average pooling 2d since max pooling usually works better than average pooling\n",
    "- Also reduced batch size to 32 from 64\n",
    "- Previous model was slow to converge so increased learning rate to 0.001 from 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51032a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second multiclass model\n",
    "model = Sequential([\n",
    "    # layer group 1\n",
    "    Conv2D(64, (3, 3), input_shape = (128, 517, 1), kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    # group 2\n",
    "    Conv2D(64, (3, 3), kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    # group 3\n",
    "    Conv2D(128, (3, 3), kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.3),\n",
    "    # group 4\n",
    "    Conv2D(256, (3, 3), kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.3),\n",
    "    # global pooling\n",
    "    GlobalMaxPooling2D(),\n",
    "    Dense(units = 128, kernel_regularizer = l2(0.0001)),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(units = num_classes, activation = 'softmax')\n",
    "], name = \"Multiclass_Model_2\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer = Adam(learning_rate = 0.001),\n",
    "    loss = CategoricalCrossentropy(label_smoothing = 0.05),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)\n",
    "\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size = 32),\n",
    "    validation_data = (X_test, y_test),\n",
    "    epochs = 100,\n",
    "    callbacks = [early_stopping],\n",
    "    class_weight = class_weights_dict\n",
    ")\n",
    "\n",
    "# result\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'\\nTest accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459b4b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# get class labels in correct order (by integer encoding)\n",
    "target_names = [label_map[i] for i in sorted(label_map.keys())]\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names = target_names, digits = 4, zero_division = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f706f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize = (12, 10))\n",
    "sns.heatmap(cm, annot = True, fmt = 'd', cmap = 'Blues', xticklabels = species_names, yticklabels = species_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xticks(rotation = 45, ha = 'right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e747d8c9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "281c2a14",
   "metadata": {},
   "source": [
    "## Another Multiclass Model with more Adjustments\n",
    "- Removed label smoothing since the data may interact with it badly\n",
    "- Removed l2 to see if it was restricting weights and hurting early learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707d5368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# third multiclass model\n",
    "model = Sequential([\n",
    "    # layer group 1\n",
    "    Conv2D(64, (3, 3), input_shape = (128, 517, 1), kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.3),\n",
    "    MaxPooling2D(2, 2),\n",
    "    # group 2\n",
    "    Conv2D(64, (3, 3), kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.3),\n",
    "    MaxPooling2D(2, 2),\n",
    "    # group 3\n",
    "    Conv2D(128, (3, 3), kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.3),\n",
    "    MaxPooling2D(2, 2),\n",
    "    # group 4\n",
    "    Conv2D(128, (3, 3), kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.3),\n",
    "    MaxPooling2D(2, 2),\n",
    "    # group 5\n",
    "    Conv2D(256, (3, 3), kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.3),\n",
    "    MaxPooling2D(2, 2),\n",
    "    # global pooling\n",
    "    GlobalMaxPooling2D(),\n",
    "    Dense(units = 256, kernel_regularizer = l2(0.0001)),\n",
    "    Activation('relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(units = num_classes, activation = 'softmax')\n",
    "], name = \"Multiclass_Model_3\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer = Adam(learning_rate = 0.0005),\n",
    "    loss = CategoricalCrossentropy(label_smoothing = 0.05),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)\n",
    "\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size = 64),\n",
    "    validation_data = (X_test, y_test),\n",
    "    epochs = 100,\n",
    "    callbacks = [early_stopping],\n",
    "    class_weight = class_weights_dict\n",
    ")\n",
    "\n",
    "# result\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'\\nTest accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bccd94d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
