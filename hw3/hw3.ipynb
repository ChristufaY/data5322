{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1ceae1b",
   "metadata": {},
   "source": [
    "## This notebook is scratch work to show proof of work. Ended up splitting this notebook into a binary and multiclass notebooky since it took too long to restart the kernel and run all cells each time I wanted to test a model. Did not want to re-run the binary models to run and test the multiclass models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b91563",
   "metadata": {},
   "source": [
    "## Import statements and loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5537f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D, BatchNormalization, SeparableConv2D, SpatialDropout2D\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, CategoricalCrossentropy\n",
    "from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018ee873",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('bird_spectrograms.hdf5', 'r')\n",
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7307ad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(f.keys()):\n",
    "    print(f[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92136096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds to ensure reproducibility \n",
    "np.random.seed(5322)\n",
    "tf.random.set_seed(5322)\n",
    "random.seed(5322)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b8e74f",
   "metadata": {},
   "source": [
    "## Binary Classification (Song Sparrow and House Sparrow)\n",
    "- Just because these two species have the most samples in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b777919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# picked the two species with most samples for binary classifcation model\n",
    "# sonspa = song sparrow, houspa = house sparrow\n",
    "X1 = np.array(f['sonspa'])\n",
    "X2 = np.array(f['houspa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91950c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create labels. 0 for song sparrow, 1 for house sparrow\n",
    "y1 = np.zeros(X1.shape[2])\n",
    "y2 = np.ones(X2.shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2538daf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine into one set\n",
    "X = np.concatenate((X1, X2), axis = 2)\n",
    "y = np.concatenate((y1, y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d1570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X has shape (128, 517, total_samples)\n",
    "# we need to format input like (total_samples, 128, 517, channels = 1 not 3 since not rgb)\n",
    "X = np.transpose(X, (2, 0, 1))\n",
    "X = X[..., np.newaxis]\n",
    "# X is now (total_samples, 128, 517, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa44ec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalization before splitting into train and set sets to avoid data leakage\n",
    "X = X.astype(np.float32)\n",
    "X = X / np.max(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1185b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 5322, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a73c86",
   "metadata": {},
   "source": [
    "## First simple model 7 total layers\n",
    "- no l2 regularization yet\n",
    "- no dropout\n",
    "- (2, 2) max pooling to take max val of 2x2 windows\n",
    "- flatten layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679de1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our first and simplest cnn for binary classification\n",
    "model = Sequential([\n",
    "    # layer group 1\n",
    "    Conv2D(16, (3,3), activation = 'relu', input_shape = (128, 517, 1)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    # layer group 2\n",
    "    Conv2D(32, (3,3), activation = 'relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    # flatten layer\n",
    "    Flatten(),\n",
    "    # dense layers\n",
    "    Dense(units = 64, activation = 'relu'),\n",
    "    Dense(units = 1, activation = 'sigmoid')\n",
    "], name = \"Binary_Model_1\")\n",
    "\n",
    "# compile model\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['accuracy', tf.keras.metrics.AUC(name = 'auc')]\n",
    ")\n",
    "\n",
    "# model summary\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 3, restore_best_weights = True)\n",
    "\n",
    "# train model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs = 100,\n",
    "    batch_size = 16,\n",
    "    validation_split = 0.2,\n",
    "    callbacks = [early_stopping]\n",
    ")\n",
    "\n",
    "# test the model\n",
    "test_loss, test_acc, test_auc = model.evaluate(X_test, y_test)\n",
    "print(f'\\nTest accuracy: {test_acc}')\n",
    "print(f'Test AUC: {test_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb05520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "\n",
    "# probabilities to binary predictions (0 or 1) using a threshold of 0.5\n",
    "y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred, zero_division = 0, target_names = ['Song Sparrow', 'House Sparrow']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c20a777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize training accuracy versus validation accuracy\n",
    "plt.plot(history.history['accuracy'], label = 'Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Binary Model 1 Training History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5d9695",
   "metadata": {},
   "source": [
    "In terms of just accuracy, this simple model does decent given its simplicity\\\n",
    "Looking deeper into the classifcation report, there is some class imbalance, which leads to no song sparrows being predicted\\\n",
    "The model did do well on the House Sparrow class which had more data samples. 100% recall and 70% precision is not bad given how basic and boilerplate the model is\\\n",
    "The 0.5 test AUC indicates it was random guessing as well\\\n",
    "Another important thing is the number of parameters in the dense layer. 7.8 million parameters is crazy given our dataset size, so add something like global average pooling 2d to reduce to a reasonable number\\\n",
    "Interesting how accuracy did not change, but the loss did slightly which did not allow early stopping\\\n",
    "Overall, decent simple model, but needs more complexity and tuning since the model stopped very early on"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac801dc7",
   "metadata": {},
   "source": [
    "## Second model with improvements/adjustments 13 total layers\n",
    "- added another layer \"group\" for complexity\n",
    "- added 0.01 l2 regularization to discourage large weights\n",
    "- added 30% dropout to help with overfitting and regularization\n",
    "- also included global average pooling 2D instead of flatten to keep the number of parameters at each layer reasonable\n",
    "- added early stopping since previous model converged early on\n",
    "- also increased the number of epochs to allow the model to possibly improve accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c36a2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our second cnn for binary classification\n",
    "model = Sequential([\n",
    "    # layer group 1\n",
    "    Conv2D(16, (3, 3), activation = 'relu', input_shape = (128, 517, 1), kernel_regularizer = l2(0.01)),\n",
    "    MaxPooling2D(pool_size = (2, 2)),\n",
    "    Dropout(rate = 0.3),\n",
    "    # layer group 2\n",
    "    Conv2D(32, (3, 3), activation = 'relu', kernel_regularizer = l2(0.01)),\n",
    "    MaxPooling2D(pool_size = (2, 2)),\n",
    "    Dropout(rate = 0.3),\n",
    "    # layer group 3\n",
    "    Conv2D(64, (3, 3), activation = 'relu', kernel_regularizer = l2(0.01)),\n",
    "    MaxPooling2D(pool_size = (2, 2)),\n",
    "    Dropout(rate = 0.3),\n",
    "    # pooling to keep # of parameters in check\n",
    "    GlobalAveragePooling2D(),\n",
    "    # dense layers\n",
    "    Dense(units = 64, activation = 'relu', kernel_regularizer = l2(0.01)),\n",
    "    Dropout(rate = 0.3),\n",
    "    Dense(units = 1, activation = 'sigmoid')\n",
    "], name = \"Binary_Model_2\")\n",
    "\n",
    "# compile model\n",
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['accuracy', tf.keras.metrics.AUC(name = 'auc')]\n",
    ")\n",
    "\n",
    "# model summary\n",
    "model.summary()\n",
    "\n",
    "# early stopping since previous model did not improve early on\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)\n",
    "\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs = 100,\n",
    "    batch_size = 32,\n",
    "    validation_split = 0.2,\n",
    "    callbacks = [early_stopping],\n",
    ")\n",
    "\n",
    "# evaluate the model\n",
    "test_loss, test_acc, test_auc = model.evaluate(X_test, y_test)\n",
    "print(f'\\nTest accuracy: {test_acc}')\n",
    "print(f'Test AUC: {test_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006e7394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "\n",
    "# probabilities to binary predictions (0 or 1) using a threshold of 0.5\n",
    "y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred, zero_division = 0, target_names = ['Song Sparrow', 'House Sparrow']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f8c117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize training accuracy versus validation accuracy\n",
    "plt.plot(history.history['accuracy'], label = 'Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Binary Model 2 Training History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc96f6b",
   "metadata": {},
   "source": [
    "Somehow did worse with adjustments in terms of accuracy and AUC. Might need to make more drastic tuning changes\\\n",
    "This model definitely overfit seen with the higher training accuracy and much lower validation accuracy\\\n",
    "The lack of improvement in validation accuracy also indicates the model is not learning well in the first place\\\n",
    "Performed worse than random guessing slightly\\\n",
    "At least this model made some predictions for both classes rather than just 1 of the 2\\\n",
    "Stopped very early on like the previous model\\\n",
    "Might need to add more layers for complexity\\\n",
    "Change regularization weight to try and prevent overfitting even more\\\n",
    "Maybe adjust dropout rate to scale with filter sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58616abb",
   "metadata": {},
   "source": [
    "## Final binary model with even more improvements/adjustments 24 total layers version 1\n",
    "- added a couple more layers for more complexity\n",
    "- started the filter size at 32 instead of 16\n",
    "- modified the dropouts at each layer to scale with filter sizes as they increase\n",
    "- change l2 regularization to 0.0001 from 0.01 to prevent overfitting a little bit\n",
    "- reduced the adam optimizer learning rate to allow for better convergence\n",
    "- added batch normalization to stabilize and speed up training a little bit given we added more layers\n",
    "- kept padding to be 'same' to retain more spatial information and details from our input as layer size increases. helps given small input sizes\n",
    "- reduced stopping patience to 3 in case continous non improvement like previous model\n",
    "- tried label smoothing so instead of comparing predictions with 0 and 1, it compares with 0.05 and 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57156fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final more complex cnn for binary classification with additional layers version 1\n",
    "model = Sequential([\n",
    "    # layer group 1\n",
    "    Conv2D(32, (3, 3), activation = 'relu', input_shape = (128, 517, 1), kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.2),\n",
    "    # layer group 2\n",
    "    Conv2D(64, (3, 3), activation = 'relu', kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.2),\n",
    "    # layer group 3\n",
    "    Conv2D(128, (3, 3), activation = 'relu', kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.3),\n",
    "    # layer group 4\n",
    "    Conv2D(128, (3, 3), activation = 'relu', kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.3),\n",
    "    # layer group 5\n",
    "    Conv2D(256, (3, 3), activation = 'relu', kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.4),\n",
    "    # keep # of params in check\n",
    "    GlobalAveragePooling2D(),\n",
    "    # dense layers\n",
    "    Dense(units = 128, activation = 'relu', kernel_regularizer = l2(0.0001)),\n",
    "    Dropout(0.4),\n",
    "    Dense(units = 1, activation = 'sigmoid')\n",
    "], name = \"Binary_Model_3\")\n",
    "\n",
    "# compile the model with lower learning rate\n",
    "model.compile(\n",
    "    optimizer = Adam(learning_rate = 0.0001),\n",
    "    loss = BinaryCrossentropy(label_smoothing = 0.05),\n",
    "    metrics = ['accuracy', tf.keras.metrics.AUC(name = 'auc')]\n",
    ")\n",
    "\n",
    "# model summary\n",
    "model.summary()\n",
    "\n",
    "# early stopping with some patience\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)\n",
    "\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs = 100,\n",
    "    batch_size = 32,\n",
    "    validation_split = 0.2,\n",
    "    callbacks = [early_stopping]\n",
    ")\n",
    "\n",
    "# evaluate the model\n",
    "test_loss, test_acc, test_auc = model.evaluate(X_test, y_test)\n",
    "print(f'\\nTest accuracy: {test_acc}')\n",
    "print(f'Test AUC: {test_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed3a77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize training accuracy versus validation accuracy\n",
    "plt.plot(history.history['accuracy'], label = 'Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Binary Model 3 Training History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4812733b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "\n",
    "# probabilities to binary predictions (0 or 1) using a threshold of 0.5\n",
    "y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred, zero_division = 0, target_names = ['Song Sparrow', 'House Sparrow']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a6f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted probabilities\n",
    "y_scores = y_pred_probs.ravel()\n",
    "\n",
    "# ROC for class 1 house sparrow\n",
    "fpr1, tpr1, _ = roc_curve(y_test, y_scores)\n",
    "auc1 = auc(fpr1, tpr1)\n",
    "\n",
    "# ROC for class 0 song sparrow\n",
    "fpr0, tpr0, _ = roc_curve(1 - y_test, 1 - y_scores)\n",
    "auc0 = auc(fpr0, tpr0)\n",
    "\n",
    "# plot both\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr1, tpr1, label = f'Class House Sparrow (AUC = {auc1:.4f})', color = 'darkorange')\n",
    "plt.plot(fpr0, tpr0, label = f'Class Song Sparrow (AUC = {auc0:.4f})', color = 'blue')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label = 'Random Guessing')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Both Classes (Binary Classification)')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51d2739",
   "metadata": {},
   "source": [
    "Much more stable model with much better accuracy than previous 2 with the gradual and progressive learning\\\n",
    "Decent accuracy --> 81% on test set\\\n",
    "Model generalizes well and stop early\\\n",
    "This model does a much better job at predicting both classes but could do better for song sparrow maybe\\\n",
    "Seems the additional layers, the label smoothing, lower l2 regularization, scaling dropout, same padding, batch normalization, and smaller learning rate helped improve the model significantly\\\n",
    "No signs of overfitting either which is amazing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f45b5a1",
   "metadata": {},
   "source": [
    "## Final binary model with even more improvements/adjustments 24 total layers version 2\n",
    "- only difference is the batch size. 32 --> 16\n",
    "- given we only had a couple thousand samples, 16 or 32 seemed like a reasonable choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1700760e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final more complex cnn for binary classification with additional layers version 2\n",
    "model = Sequential([\n",
    "    # layer group 1\n",
    "    Conv2D(32, (3, 3), activation = 'relu', input_shape = (128, 517, 1), kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.2),\n",
    "    # layer group 2\n",
    "    Conv2D(64, (3, 3), activation = 'relu', kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.2),\n",
    "    # layer group 3\n",
    "    Conv2D(128, (3, 3), activation = 'relu', kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.3),\n",
    "    # layer group 4\n",
    "    Conv2D(128, (3, 3), activation = 'relu', kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.3),\n",
    "    # layer group 5\n",
    "    Conv2D(256, (3, 3), activation = 'relu', kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.4),\n",
    "    # keep # of params in check\n",
    "    GlobalAveragePooling2D(),\n",
    "    # dense layers\n",
    "    Dense(units = 128, activation = 'relu', kernel_regularizer = l2(0.0001)),\n",
    "    Dropout(0.4),\n",
    "    Dense(units = 1, activation = 'sigmoid')\n",
    "], name = \"Binary_Model_4\")\n",
    "\n",
    "# compile the model with lower learning rate\n",
    "model.compile(\n",
    "    optimizer = Adam(learning_rate = 0.0001),\n",
    "    loss = BinaryCrossentropy(label_smoothing = 0.05),\n",
    "    metrics = ['accuracy', tf.keras.metrics.AUC(name = 'auc')]\n",
    ")\n",
    "\n",
    "# model summary\n",
    "model.summary()\n",
    "\n",
    "# early stopping with some patience\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)\n",
    "\n",
    "# train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs = 100,\n",
    "    batch_size = 16,\n",
    "    validation_split = 0.2,\n",
    "    callbacks = [early_stopping]\n",
    ")\n",
    "\n",
    "# evaluate the model\n",
    "test_loss, test_acc, test_auc = model.evaluate(X_test, y_test)\n",
    "print(f'\\nTest accuracy: {test_acc}')\n",
    "print(f'Test AUC: {test_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34b04ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize training accuracy versus validation accuracy\n",
    "plt.plot(history.history['accuracy'], label = 'Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Binary Model 4 Training History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52749fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "\n",
    "# probabilities to binary predictions (0 or 1) using a threshold of 0.5\n",
    "y_pred = (y_pred_probs >= 0.5).astype(int)\n",
    "\n",
    "print(classification_report(y_test, y_pred, zero_division = 0, target_names = ['Song Sparrow', 'House Sparrow']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620b7e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted probabilities\n",
    "y_scores = y_pred_probs.ravel()\n",
    "\n",
    "# ROC for class 1 house sparrow\n",
    "fpr1, tpr1, _ = roc_curve(y_test, y_scores)\n",
    "auc1 = auc(fpr1, tpr1)\n",
    "\n",
    "# ROC for class 0 song sparrow\n",
    "fpr0, tpr0, _ = roc_curve(1 - y_test, 1 - y_scores)\n",
    "auc0 = auc(fpr0, tpr0)\n",
    "\n",
    "# plot both\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr1, tpr1, label = f'Class House Sparrow (AUC = {auc1:.4f})', color = 'darkorange')\n",
    "plt.plot(fpr0, tpr0, label = f'Class Song Sparrow (AUC = {auc0:.4f})', color = 'blue')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label = 'Random Guessing')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Both Classes (Binary Classification)')\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ae69f3",
   "metadata": {},
   "source": [
    "A little bit better. Reducing batch size got us ~3% increase in accuracy and AUC\\\n",
    "Also did better in terms of precision and recall for each class\\\n",
    "No signs of overfitting either. The training and validation accuracies were similar to one another which is good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1def19",
   "metadata": {},
   "source": [
    "## Multiclass Classification (All species)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e02269f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_list = []\n",
    "y_list = []\n",
    "\n",
    "# loop through each species' data\n",
    "for species in list(f.keys()):\n",
    "    spectrograms = np.array(f[species])  # (128, 517, N)\n",
    "    n_samples = spectrograms.shape[2]\n",
    "    labels = [species] * n_samples\n",
    "\n",
    "    X_list.append(spectrograms)\n",
    "    y_list.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a84cde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack and reshape\n",
    "X = np.concatenate(X_list, axis = 2)      # (128, 517, total_samples)\n",
    "X = np.transpose(X, (2, 0, 1))           # (samples, 128, 517)\n",
    "X = X[..., np.newaxis]                   # (samples, 128, 517, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4914b6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize spectrograms to [0, 1]\n",
    "X = X / np.max(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216e7a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels to arrays\n",
    "y = np.array(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86dc47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode species to ints\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)          # e.g., 'amecro' → 0, 'amerob' → 1, etc.\n",
    "label_map = dict(zip(le.transform(le.classes_), le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc900a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode\n",
    "y_onehot = to_categorical(y_encoded)     # shape: (samples, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b235bc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test. stratify to balance species\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_onehot, test_size = 0.2, random_state = 5322, stratify = y_encoded\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d2395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also get y_test_raw = y_encoded to decode predictions later and for plotting\n",
    "_, y_test_raw = train_test_split(\n",
    "    y_encoded, test_size = 0.2, random_state = 5322, stratify = y_encoded\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32804af",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fe1d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(le.classes_)\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64774a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first multiclass model\n",
    "model = Sequential([\n",
    "    # layer group 1\n",
    "    Conv2D(32, (3, 3), activation = 'relu', input_shape = (128, 517, 1), kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    SpatialDropout2D(0.3),\n",
    "    # group 2\n",
    "    Conv2D(64, (3, 3), activation = 'relu', kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    SpatialDropout2D(0.3),\n",
    "    # group 3\n",
    "    SeparableConv2D(128, (3, 3), activation = 'relu', kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.4),\n",
    "    # group 4\n",
    "    SeparableConv2D(128, (3, 3), activation = 'relu', kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.4),\n",
    "    # group 5\n",
    "    SeparableConv2D(256, (3, 3), activation = 'relu', kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.5),\n",
    "    # global pooling\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(units = 128, activation = 'relu', kernel_regularizer = l2(0.0001)),\n",
    "    Dropout(0.5),\n",
    "    Dense(units = num_classes, activation = 'softmax')\n",
    "], name = \"Multiclass_Model_1\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer = Adam(learning_rate = 0.0001),\n",
    "    loss = CategoricalCrossentropy(),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)\n",
    "\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size = 16),\n",
    "    validation_data = (X_test, y_test),\n",
    "    epochs = 100,\n",
    "    # batch_size = 16,\n",
    "    # validation_split = 0.2,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'\\nTest accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40322894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# get class labels in correct order (by integer encoding)\n",
    "target_names = [label_map[i] for i in sorted(label_map.keys())]\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names = target_names, digits = 4, zero_division = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee79d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize = (10, 8))\n",
    "sns.heatmap(cm, annot = True, fmt = 'd', cmap = 'Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc84b36",
   "metadata": {},
   "source": [
    "Did not do as well as I thought it would."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15dd0d40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e904c1a",
   "metadata": {},
   "source": [
    "## Multiclass Classification with adjustments\n",
    "- added label smoothing of 0.05\n",
    "- increased batch size to 64 from 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d097ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first multiclass model\n",
    "model = Sequential([\n",
    "    # layer group 1\n",
    "    Conv2D(32, (3, 3), activation = 'relu', input_shape = (128, 517, 1), kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.2),\n",
    "    # group 2\n",
    "    Conv2D(64, (3, 3), activation = 'relu', kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.2),\n",
    "    # group 3\n",
    "    Conv2D(128, (3, 3), activation = 'relu', kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.3),\n",
    "    # group 4\n",
    "    Conv2D(128, (3, 3), activation = 'relu', kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.3),\n",
    "    # group 5\n",
    "    Conv2D(256, (3, 3), activation = 'relu', kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.4),\n",
    "    # global pooling\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(units = 256, activation = 'relu', kernel_regularizer = l2(0.0001)),\n",
    "    Dropout(0.3),\n",
    "    Dense(units = 128, activation = 'relu', kernel_regularizer = l2(0.0001)),\n",
    "    Dropout(0.4),\n",
    "    Dense(units = num_classes, activation = 'softmax')\n",
    "], name = \"Multiclass_Model_2\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer = Adam(learning_rate = 0.0001),\n",
    "    loss = CategoricalCrossentropy(label_smoothing = 0.05),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs = 100,\n",
    "    batch_size = 64,\n",
    "    validation_split = 0.2,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'\\nTest accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43317ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first multiclass model\n",
    "model = Sequential([\n",
    "    # layer group 1\n",
    "    Conv2D(32, (3, 3), activation = 'relu', input_shape = (128, 517, 1), kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.2),\n",
    "    # group 2\n",
    "    Conv2D(64, (3, 3), activation = 'relu', kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.2),\n",
    "    # group 3\n",
    "    Conv2D(128, (3, 3), activation = 'relu', kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.3),\n",
    "    # group 4\n",
    "    Conv2D(128, (3, 3), activation = 'relu', kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.3),\n",
    "    # group 5\n",
    "    Conv2D(256, (3, 3), activation = 'relu', kernel_regularizer = l2(0.0001), padding = 'same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.4),\n",
    "    # global pooling\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dense(units = 256, activation = 'relu', kernel_regularizer = l2(0.0001)),\n",
    "    Dropout(0.3),\n",
    "    Dense(units = 128, activation = 'relu', kernel_regularizer = l2(0.0001)),\n",
    "    Dropout(0.4),\n",
    "    Dense(units = num_classes, activation = 'softmax')\n",
    "], name = \"Multiclass_Model_1\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer = Adam(learning_rate = 0.0001),\n",
    "    loss = CategoricalCrossentropy(label_smoothing = 0.05),\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs = 100,\n",
    "    batch_size = 16,\n",
    "    validation_split = 0.2,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluation\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'\\nTest accuracy: {test_acc}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
